<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Lesson 4: From neural responses to stimulus features" href="lesson4.html" /><link rel="prev" title="Lesson 2: Linear encoding models" href="lesson2.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>Lesson 3: Building the likelihood (by fitting the covariance matrix) - Braincoder documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=362ab14a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=ca2c29f3" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/fontawesome.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/solid.min.css" />
    <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/brands.min.css" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Braincoder  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  
  <span class="sidebar-brand-text">Braincoder  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1 current has-children"><a class="reference internal" href="index.html">Tutorial</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Tutorial</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="lesson1.html">Lesson 1: Encoding models</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson2.html">Lesson 2: Linear encoding models</a></li>
<li class="toctree-l2 current current-page"><a class="current reference internal" href="#">Lesson 3: Building the likelihood (by fitting the covariance matrix)</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson4.html">Lesson 4: From neural responses to stimulus features</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson5.html">Lesson 5: Decoding noisy neural data</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson6.html">Lesson 6: Decoding two-dimensional stimulus spaces</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson6.html#decode-the-marginal-probability-distribution">Decode the marginal probability distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson6.html#decode-the-conditional-probability-distribution">Decode the conditional probability distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="lesson6.html#the-complex-plane">The complex plane</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/00_encodingdecoding/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/fit_residuals.html">Fit the residual covariance matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/linear_encoding_model.html">Linear encoding model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/encoding_model.html">Create a simple Gaussian Prf encoding model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/invert_model.html">Invert encoding model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/decode.html">Decoding of stimuli from neural data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/00_encodingdecoding/decode2d.html">Decoding 2D stimuli from neural data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">General bibliography</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="lesson-3-building-the-likelihood-by-fitting-the-covariance-matrix">
<span id="tutorial-lesson3"></span><h1>Lesson 3: Building the likelihood (by fitting the covariance matrix)<a class="headerlink" href="#lesson-3-building-the-likelihood-by-fitting-the-covariance-matrix" title="Link to this heading">#</a></h1>
<p>For many neuroscientific questions, we are interested in the relationship between neural codes
and objective stimulus featuers. For example, we might want to know how the brain represents
numbers, orientatoins, or spatial positions, and how these representatios change as a function
of task demands, attention, or prior expectations.</p>
<p>One particular powerful approach is to <em>decode</em> stimulus features from neural data
in a Bayesian fashion (e.g., van Bergen et al., 2015; Baretto-Garcia et al, 2023).</p>
<section id="inverting-the-encoding-models">
<h2>Inverting the encoding models<a class="headerlink" href="#inverting-the-encoding-models" title="Link to this heading">#</a></h2>
<p>Here, we show how we go from a <strong>determistic</strong> forward model (i.e., a model that predicts neural
responses from stimulus features) to a <strong>probabilistic</strong> inverse model (i.e., a model that
predicts stimulus features from neural responses). We will do so using a <strong>Bayesian inversion scheme</strong>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[p(s | x; \theta) = \frac{p(x | s; \theta) p(s)}{p(x)}\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(s\)</span> is a n-dimensional point in stimulus space, and <span class="math notranslate nohighlight">\(x\)</span> is a n-dimensional
activation pattern in neural space, and <span class="math notranslate nohighlight">\(p(s | x; \theta)\)</span> is the posterior probability of
stimulus <span class="math notranslate nohighlight">\(s\)</span> given neural response <span class="math notranslate nohighlight">\(x\)</span> and model parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</section>
<section id="a-multivariate-likelihood-function">
<h2>A multivariate likelihood function<a class="headerlink" href="#a-multivariate-likelihood-function" title="Link to this heading">#</a></h2>
<p>The crucial element that is still lacking for this Bayesian inversion scheme is a <strong>likelihood function</strong>.
Note stanard encoding models do not have a likelihood function, because they are deterministic
(<span class="math notranslate nohighlight">\(f(x;\theta): s \mapsto x\)</span>).
They give us the average neural respons to a certain stimulus, but they don’t tell us how likely a certain
neural response is, given a certain stimulus.
However, we can easily derive a likelihood function from the forward model by adding Gaussian noise:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[p(x | s; \theta) = f(s; \theta) + \epsilon\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a <em>multivariate Normal distribution</em></p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\epsilon \sim \mathcal{N}(0, \Sigma)\]</div>
</div>
</section>
<section id="estimating-sigma">
<h2>Estimating <span class="math notranslate nohighlight">\(\Sigma\)</span><a class="headerlink" href="#estimating-sigma" title="Link to this heading">#</a></h2>
<section id="the-problem-with-high-dimensional-covariance-matrices">
<h3>The problem with high-dimensional covariance matrices<a class="headerlink" href="#the-problem-with-high-dimensional-covariance-matrices" title="Link to this heading">#</a></h3>
<p>How to set the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>?
One approach would be to assume independent noise across neural dimensions (e.g.,
fMRI voxels), and use a spherical covariance matrix <span class="math notranslate nohighlight">\(\Sigma = \tau^T\tau I\)</span>, where
<span class="math notranslate nohighlight">\(\tau\)</span> is a vector containing the standard deviation of the residuals of the encoding models
and I is the identity matrix.
However, if there <em>is</em> substantial covariance between the noise terms of different neural
dimensions (i.e., voxels), this could have severe consequences for the decoding performance.
In particular, the posterior might be overly confident in its predictions, assuming
independt sources of information that are not. Under some circumstances, the mean posterior,
the point estimate of the decoded stimulus, can also be affected.
Van Bergen et al. (2015, 2017) showed that modeling some of the covariance is ineed crucial
for making correct inferences about neural data.
However, we generally have a large number of voxels and very limited data.
Therefore, estimating the full covariance
matrix is not feasible (note that the number of free parameters scales quadratically with
the number of voxels <span class="math notranslate nohighlight">\(p= n \times \frac{n-1}{2}\)</span>). Note that this is a general
problem of estimating covariance, and not specific to our use case (e.g., Ledoit, …).</p>
<p>Van Bergen et al. (2015, 2017) proposed a two-part solution.</p>
</section>
<section id="regularizing-the-covariance-matrix">
<h3>Regularizing the covariance matrix<a class="headerlink" href="#regularizing-the-covariance-matrix" title="Link to this heading">#</a></h3>
<p>The first proposal of van Bergen et al. (2015, 2017), based on the work of
Ledoit (…), is to use a <em>shrinkage estimator</em> to estimate the covariance matrix.
Specifically, the free parameter <span class="math notranslate nohighlight">\(\rho\)</span> scales between a perfectly-correlated
covariance matrix (<span class="math notranslate nohighlight">\(\rho = 1\)</span>) and a diagonal covariance matrix (<span class="math notranslate nohighlight">\(\rho = 0\)</span>):</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\Sigma = \rho \tau^T\tau + (1-\rho) \tau^T\tau I\]</div>
</div>
</section>
<section id="accounting-for-shared-noise-due-to-shared-neural-populations">
<h3>Accounting for shared noise due to shared neural populations<a class="headerlink" href="#accounting-for-shared-noise-due-to-shared-neural-populations" title="Link to this heading">#</a></h3>
<p>Van Bergen et al. (2015) also note that <em>voxels that share more tuned neural populations</em>
should also be more highly correlated. They do so by adding a second term to the covariance
matrix, which is based on the similarity of the tuning curves of the voxels,
given by the weight matrix <span class="math notranslate nohighlight">\(W\)</span>.
Recall that W is a <span class="math notranslate nohighlight">\(m \times n\)</span> matrix, where <span class="math notranslate nohighlight">\(n\)</span> is the number of voxels,
and <span class="math notranslate nohighlight">\(m\)</span> is the number of neural populations. So <span class="math notranslate nohighlight">\(WW^T\)</span> is a <span class="math notranslate nohighlight">\(n \times n\)</span>
matrix that contains the similarity between all pairs of voxels. We scale this
matrix by the free parameters <span class="math notranslate nohighlight">\(\sigma^2\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\sigma^2 WW^T\]</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You might realize now that non-linear encoding models, like a standard
<code class="docutils literal notranslate"><span class="pre">GaussianPRFModel</span></code> does not have a weight matrix <span class="math notranslate nohighlight">\(W\)</span>. There are
two work arounds here:
* Use the identity matrix <span class="math notranslate nohighlight">\(I\)</span> as weight-matrix. This corresponds to saying that all neural populations in all the voxels are unique and share no noise sources.
* Rewrite the individual encoding functions <span class="math notranslate nohighlight">\(f(\theta_{1..i})\)</span> to linear functions that can be interpreted as a weight matrix <span class="math notranslate nohighlight">\(W\)</span>. For PRF models, we can just set up a grid of plausible stimulus values and set the height of the PRF as the weight of those stimulus values. The weight matrix <span class="math notranslate nohighlight">\(W\)</span> effectively then describes the amount of overlap between the receptive fields of different voxels. The noise covariance is then assumed to scale with the amount of overlap in recpeptive fields, which seems quite plausible and has worked well in earlier work (e.g., Baretto-Garcia et al., 2023).</p>
</div>
<p>The complete formula for the covariance matrix is thus:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\Sigma = \rho \tau^T\tau + (1-\rho) \tau^T\tau I  + \sigma^2 W^TW\]</div>
</div>
<p>Thus, the <span class="math notranslate nohighlight">\(n \times n`covariance-matrix :math:\)</span>Sigma` is now described by
<span class="math notranslate nohighlight">\(n + 2\)</span> parameters (the <span class="math notranslate nohighlight">\(\tau\)</span> noise vector of length
<span class="math notranslate nohighlight">\(n\)</span> plus <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>.</p>
</section>
<section id="using-the-sample-covariance">
<h3>Using the sample covariance<a class="headerlink" href="#using-the-sample-covariance" title="Link to this heading">#</a></h3>
<p>Note that additional elements can be added to the covariance matrix.
For one, we can add a proportion <span class="math notranslate nohighlight">\(\lambda\)</span> of the empirical
noise covariance matrix <span class="math notranslate nohighlight">\(S\)</span> to the regularized covariance matrix, to allow for
a more sophisticated noise model:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\Sigma' = (1-\lambda) \cdot \Sigma + \lambda S\]</div>
</div>
</section>
<section id="using-the-anatomical-distance">
<h3>Using the anatomical distance<a class="headerlink" href="#using-the-anatomical-distance" title="Link to this heading">#</a></h3>
<p>Similarly, one could add a term that accounts for the physical
distance between different neural sources (i.e., voxels).</p>
</section>
</section>
<section id="fitting-sigma">
<h2>Fitting <span class="math notranslate nohighlight">\(\Sigma\)</span><a class="headerlink" href="#fitting-sigma" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">braincoder</span></code> contains the <code class="docutils literal notranslate"><span class="pre">ResidualFitter</span></code>-class that can be used
to fit a noise covariance matrix, and thereby a likelihood function
to a fitted <code class="docutils literal notranslate"><span class="pre">EncodingModel</span></code>.</p>
<p>Here we first set up a simple <code class="docutils literal notranslate"><span class="pre">VonmisesPRF</span></code>-model and simulate some data
with covarying noise.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">braincoder.models</span> <span class="kn">import</span> <span class="n">VonMisesPRF</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Set up six evenly spaced von Mises PRFs</span>
<span class="n">centers</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;mu&#39;</span><span class="p">:</span><span class="n">centers</span><span class="p">,</span> <span class="s1">&#39;kappa&#39;</span><span class="p">:</span><span class="mf">1.</span><span class="p">,</span> <span class="s1">&#39;amplitude&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;baseline&#39;</span><span class="p">:</span><span class="mf">0.0</span><span class="p">},</span>
                          <span class="n">index</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">([</span><span class="sa">f</span><span class="s1">&#39;Voxel </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;voxel&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># We have only 3 voxels, each with a linear combination of the 6 von Mises functions:</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                    <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">VonMisesPRF</span><span class="p">(</span><span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="c1"># 50 random orientations</span>
<span class="n">paradigm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="mi">2</span>

<span class="c1"># Arbitrary covariance matrix</span>
<span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">.25</span><span class="p">,</span> <span class="mf">.75</span><span class="p">,</span> <span class="mf">.25</span><span class="p">],</span>
       <span class="p">[</span><span class="mf">.25</span><span class="p">,</span> <span class="mf">.25</span><span class="p">,</span> <span class="mf">.75</span><span class="p">]])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">simulate</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">paradigm</span><span class="o">=</span><span class="n">paradigm</span><span class="p">)</span>

</pre></div>
</div>
<p>Now we can import the <cite>ResidualFitter</cite> and estimate <span class="math notranslate nohighlight">\(\Sigma\)</span> (here called
<cite>omega</cite> for legacy reasons):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import ResidualFitter</span>
<span class="kn">from</span> <span class="nn">braincoder.optimize</span> <span class="kn">import</span> <span class="n">ResidualFitter</span>

<span class="n">fitter</span> <span class="o">=</span> <span class="n">ResidualFitter</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">paradigm</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># omega is the covariance matrix, dof can be estimated when a</span>
<span class="c1"># multivariate t-distribution (rather than a normal distribution)</span>
<span class="c1"># is used</span>
<span class="n">omega</span><span class="p">,</span> <span class="n">dof</span> <span class="o">=</span> <span class="n">fitter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">progressbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">omega</span><span class="p">)</span>

<span class="c1"># %%</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(50, 3)
init_tau: 0.717453122138977, 0.8030033707618713
USING A PSEUDO-WWT!
WWT max: 4.0
[[0.5844659  0.12335697 0.18236224]
 [0.12335697 0.751479   0.1024456 ]
 [0.18236224 0.1024456  0.5217994 ]]
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here we have used the <em>generating</em> parameters and weights to fit the
<code class="docutils literal notranslate"><span class="pre">omega</span></code>-matrix. Note that in real data, we would use estimated parameters
and/or weights.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The complete Python script and its output can be found <a class="reference internal" href="../auto_examples/00_encodingdecoding/fit_residuals.html#sphx-glr-auto-examples-00-encodingdecoding-fit-residuals-py"><span class="std std-ref">here</span></a>.</p>
</div>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<dl>
<dt>In this lesson we have seen:</dt><dd><ul class="simple">
<li><p>We need to add a noise model to the classical encoding models <span class="math notranslate nohighlight">\(f(s, \theta): s \mapsto x\)</span> to get a <strong>likelihood function</strong> which we can invert.</p></li>
<li><p>Conctretely, we add a multivariate Gaussian noise model to the deterministic predictions of the encodig models</p></li>
<li><p>We need a noise covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> (or <code class="docutils literal notranslate"><span class="pre">omega</span></code>) for this to work.</p></li>
<li><p>We use a regularised estimate of the covariance matrix.</p></li>
</ul>
<p>In the:ref:<cite>next lesson&lt;tutorial_lesson4&gt;</cite>, we will further explore how we can use a likelihood
function to map from neural responses to stimulus features.</p>
</dd>
</dl>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="lesson4.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Lesson 4: From neural responses to stimulus features</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="lesson2.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Lesson 2: Linear encoding models</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Gilles de Hollander
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Lesson 3: Building the likelihood (by fitting the covariance matrix)</a><ul>
<li><a class="reference internal" href="#inverting-the-encoding-models">Inverting the encoding models</a></li>
<li><a class="reference internal" href="#a-multivariate-likelihood-function">A multivariate likelihood function</a></li>
<li><a class="reference internal" href="#estimating-sigma">Estimating <span class="math notranslate nohighlight">\(\Sigma\)</span></a><ul>
<li><a class="reference internal" href="#the-problem-with-high-dimensional-covariance-matrices">The problem with high-dimensional covariance matrices</a></li>
<li><a class="reference internal" href="#regularizing-the-covariance-matrix">Regularizing the covariance matrix</a></li>
<li><a class="reference internal" href="#accounting-for-shared-noise-due-to-shared-neural-populations">Accounting for shared noise due to shared neural populations</a></li>
<li><a class="reference internal" href="#using-the-sample-covariance">Using the sample covariance</a></li>
<li><a class="reference internal" href="#using-the-anatomical-distance">Using the anatomical distance</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fitting-sigma">Fitting <span class="math notranslate nohighlight">\(\Sigma\)</span></a></li>
<li><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>