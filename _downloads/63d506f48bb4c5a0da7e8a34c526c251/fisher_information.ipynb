{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fisher information to estimate precision of encoding parameters across stimulus space\n\nFisher information is defined as the expected value of the square of the derivative of the log-likelihood\nwith respect to some parameter.\n\n\n\\begin{align}I(\\theta) = E[ (\\frac{\\partial}{\\partial \\theta} \\log f(X; \\theta) )^2 ]\\end{align}\n\nIn our case, the parameter $\\theta$ might be the stimulus (dimension) we are interested in. What the Fisher information\nnow gives us, \\_given a fitted encoding *and* noise model\\_, is the expected precision of the estimated\nparameter across the stimulus space.\n\nIn other words, using the Fisher information, we can already estimate how well we can estimate the stimulus\nat different locations in stimulus space without actually doing any decoding.\n\nNotably, Fisher information also plays a crucial role in efficient coding theory. In general, the expected\nsquared error of a likelihood function, given a limited 'information budget',\ncan be minimized by using a likelihood function where the Fisher information is proportional\nto the derivative of the cummulative prior distribution (:cite:t:`wei2015bayesian`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\nfrom braincoder.models import GaussianPRF\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as ss\nimport seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up generating model\nLet's set up a Gaussian PRF on the line that generates some data. \nWe put the centers of the PRFs equally across the line, using a\nthe quantiles of a *uniform prior distribution*\n(i.e. the quantiles of a uniform distribution are equally spaced).\n%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# 8 points on the uniform distribution between 0 and 10\nmus = ss.uniform(0, 10.).ppf(np.linspace(0.0, 1.0, 10)[1:-1])\nparameters = pd.DataFrame({'mu':mus, 'sd':1., 'baseline':0.0, 'amplitude':1.0})[['mu', 'sd', 'amplitude', 'baseline']]\n\n# Let's go over the entire line from 0 to 10\nparadigm = pd.Series(np.linspace(-2, 12, 100, dtype=np.float32))\n\n# For now let's assume the noise is spherical\nomega = np.identity(len(parameters)).astype(np.float32)\n\nmodel = GaussianPRF(parameters=parameters, paradigm=paradigm)\ndata = model.predict(paradigm=paradigm).set_index(pd.Index(paradigm, name='stimulus'))\ndata = data.stack().to_frame('strength')\ndata.index.set_names('rf', level=1, inplace=True)\n\nsns.lineplot(data=data.reset_index(), x='stimulus', y='strength', hue='rf', palette=['k'], alpha=0.5, legend=False)\nsns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Fisher Information\nNow that we have the generating model, we can calculate the Fisher information\nfor each stimulus point. We can do this by taking the expectation of the\nsecond derivative of the log-likelihood with respect to the stimulus.\nthe fucntion `Model.get_fisher_information` does this for us by sampling\nfrom the noise distribution defined by `omega` and calculating the second\nderivative of the log-likelihood with respect to the stimulus using autodiff.\n%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fisher_information = model.get_fisher_information(stimuli=np.linspace(-5, 15, 1000).astype(np.float32), omega=omega, dof=None, n=5000).to_frame()\n\nsns.lineplot(x='stimulus', y='Fisher information', data=fisher_information.reset_index(), color='k')\nsns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the Fisher information is largest at the center of the PRFs and\ndecreases towards the edges. This is because the PRFs are more overlapping at the\ncenter and less overlapping at the edges. This is also reflected in the Fisher\ninformation, which is a measure of the expected precision of the estimated stimulus\nat each point in stimulus space.\nOutside of the receptive fields, the Fisher information is 0: when stimuli occur\noutside of the receptive fields, the likelihood function is flat and the Fisher\ninformation is 0.\n%%\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that if we include some correlation in the errors\nof the receptive fields, under some conditions,\nthe Fisher information is higher.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "omega_correlated = omega + .5 - .5*np.identity(len(parameters))\n\nfisher_information_uncorrelated = model.get_fisher_information(stimuli=np.linspace(-5, 15, 1000).astype(np.float32), omega=omega.astype(np.float32), dof=None, n=5000).to_frame()\nfisher_information_correlated = model.get_fisher_information(stimuli=np.linspace(-5, 15, 1000).astype(np.float32), omega=omega_correlated.astype(np.float32), dof=None, n=5000).to_frame()\n\nfisher_information = pd.concat((fisher_information_uncorrelated, fisher_information_correlated), axis=0, keys=['uncorrelated', 'correlated'], names=['correlation'])\n\nsns.lineplot(x='stimulus', y='Fisher information', data=fisher_information.reset_index(), color='k', style='correlation')\nsns.despine()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also validate the Fisher information by decoding and looking \nat the errors of the decoded stimuli. The Fisher information should\nbe inversely proportional to the expected squared error of the decoded\nstimuli.\n%%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nparadigm = np.repeat(np.linspace(-2, 12, 50, dtype=np.float32), 100)\nsimulated_data = model.simulate(paradigm=paradigm, noise=omega)\nomega = np.identity(len(parameters)).astype(np.float32)\n\n\n\npdf = model.get_stimulus_pdf(simulated_data, np.linspace(-5, 15, 100), omega=omega)\nE = np.trapz(pdf*pdf.columns.values[np.newaxis, :], x=pdf.columns, axis=1)\nerror = np.sqrt((paradigm - E)**2)\nerror = pd.Series(error, index=pd.Index(paradigm, name='stimulus')).to_frame('error')\nsns.lineplot(x='stimulus', y='error', data=error)\nsns.despine()\nplt.title('Objective decoding error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "same goes for the variance of the decoded posterior\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "posterior_variance = np.trapz(pdf*(pdf.columns.values[np.newaxis, :] - E[:, np.newaxis])**2, x=pdf.columns, axis=1)\n\nerror['posterior sd'] = np.sqrt(posterior_variance)\nplt.figure()\nsns.lineplot(x='stimulus', y='posterior sd', data=error.reset_index())\nsns.despine()\nplt.title('Posterior std.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fisher information for different RF distributions\nLet's calculate the Fisher information for different RF structures\n%\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Centered distribution around 5.\nmus1 = ss.norm(5, 1.).ppf(np.linspace(1e-4, 1-1e-4, 10)[1:-1])\n\n# 8 points on the uniform distribution between 0 and 10\nmus2 = ss.uniform(0, 10).ppf(np.linspace(1e-4, 1-1e-4, 10)[1:-1])\nparameters_sets = [{'mu':mus1, 'sd':1.5, 'baseline':0.0, 'amplitude':1.0},\n                   {'mu':mus2, 'sd':1.5, 'baseline':0.0, 'amplitude':1.0},\n                   {'mu':mus2, 'sd':.75, 'baseline':0.0, 'amplitude':1.0},\n                   {'mu':mus2, 'sd':1.5, 'baseline':0.0, 'amplitude':.5}]\n\nparameter_sets = pd.concat([pd.DataFrame(p)[['mu', 'sd', 'amplitude', 'baseline']] for p in parameters_sets], keys=range(4), names=['set', 'parameter'], axis=0)\n\n\nset_labels = ['Unequally distributed mus', 'Equally distributed mus', 'Smaller dispersion', 'Smaller amplitude (vs noise)' ]\n\n\n# Make predictions\npreds = []\nfor set, pars in parameter_sets.groupby('set'):\n    model = GaussianPRF(parameters=pars.droplevel(0), paradigm=paradigm)\n    pred = model.predict(paradigm=paradigm).set_index(pd.Index(paradigm, name='stimulus'))\n    preds.append(pred)\n\npreds = pd.concat(preds, keys=set_labels, axis=0, names=['set'])\n\n# Calculate Fisher information\nfis = []\n\nstimuli = np.linspace(-5, 15, 200, dtype=np.float32)\nfor set, pars in parameter_sets.groupby('set'):\n    model = GaussianPRF(parameters=pars.droplevel(0), paradigm=paradigm)\n    omega = np.identity(len(pars)).astype(np.float32)\n    fi = model.get_fisher_information(stimuli=stimuli, omega=omega, n=10000).to_frame(\"fisher information\").set_index(pd.Index(stimuli, name='stimulus'))\n    fis.append(fi)\n\nfis = pd.concat(fis, keys=set_labels, axis=0, names=['set'])\n\n\n# Plot the receptive fields and the Fisher information\ntmp = preds.stack().to_frame('strength').join(fis)\ng = sns.FacetGrid(col='set', col_wrap=2, data=tmp.reset_index(), sharey=True, col_order=set_labels)\ng.map_dataframe(sns.lineplot, 'stimulus', 'strength', hue='parameter', palette=['k'], alpha=0.5)\ng.set_titles('{col_name}')\ng.figure.suptitle('Receptive field distributions')\ng.figure.subplots_adjust(top=.9)\n\ng2 = sns.FacetGrid(col='set', col_wrap=2, data=fis.reset_index(), sharey=True, col_order=set_labels)\ng2.map(sns.lineplot, 'stimulus', 'fisher information', color='r')\ng2.add_legend()\ng2.set_titles('{col_name}')\ng2.figure.suptitle('Fisher information')\ng2.figure.subplots_adjust(top=.9)\n\ng3 = sns.FacetGrid(hue='set', data=fis.reset_index(), sharey=True)\ng3.map(sns.lineplot, 'stimulus', 'fisher information')\ng3.add_legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another intersting phenomenon we can now study is how different\nbasis functions/receptive fields modulate the Fisher information.\nFor example, it is well-known that numerical receptive fields in\nparietal cortex are shaped as a log-normal distribution. We can\nnow study how the Fisher information is modulated by the shape of\nthe receptive fields.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braincoder.models import LogGaussianPRF\nmus = np.linspace(5, 25, 8)\n\nparameters = pd.DataFrame({'mu':mus, 'sd':25., 'baseline':0.0, 'amplitude':1.0})[['mu', 'sd', 'amplitude', 'baseline']]\n\n# Set up model and paradigm and plot the receptive fields\nparadigm = np.linspace(0, 100, 100, dtype=np.float32)\nmodel = LogGaussianPRF(parameters=parameters, paradigm=paradigm)\ny = model.predict(paradigm=paradigm)\n\ny.plot(c='k', legend=False, alpha=0.5)\nsns.despine()\nplt.title('Receptive fields')\n\n# Calculate Fisher information\nomega = np.identity(len(parameters)).astype(np.float32)\nfisher_information = model.get_fisher_information(stimuli=np.linspace(5, 100, 1000).astype(np.float32), omega=omega, n=5000).to_frame()\n\nfisher_information['sqrt(fi)'] = np.sqrt(fisher_information['Fisher information'])\n\nplt.figure()\nsns.lineplot(x='stimulus', y='sqrt(fi)', data=fisher_information.reset_index(), color='k')\nplt.title('Square root of Fisher information')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Somewhat consistent with the literature, the Fisher information\nbehaves roughly like 1/x now, where x is the stimulus. This means that the\nprecision of the estimated stimulus decreases with increasing\nstimulus value. \n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}