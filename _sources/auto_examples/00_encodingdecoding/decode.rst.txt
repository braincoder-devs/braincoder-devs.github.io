
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/00_encodingdecoding/decode.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_00_encodingdecoding_decode.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_00_encodingdecoding_decode.py:


Decoding of stimuli from neural data
=============================================

Here we will simulate neural data given a ground truth encoding model
 and try to decode the stimulus from the data.

.. GENERATED FROM PYTHON SOURCE LINES 8-33

.. code-block:: Python


    # Set up a neural model
    from braincoder.models import GaussianPRF
    import numpy as np
    import pandas as pd
    import scipy.stats as ss

    # Set up 100 random of PRF parameters
    n = 20
    n_trials = 50
    noise = 1.

    mu = np.random.rand(n) * 100
    sd = np.random.rand(n) * 45 + 5
    amplitude = np.random.rand(n) * 5
    baseline = np.random.rand(n) * 2 - 1

    parameters = pd.DataFrame({'mu':mu, 'sd':sd, 'amplitude':amplitude, 'baseline':baseline})

    # We have a paradigm of random numbers between 0 and 100
    paradigm = np.ceil(np.random.rand(n_trials) * 100)

    model = GaussianPRF(parameters=parameters)
    data = model.simulate(paradigm=paradigm, noise=noise)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [61. 60. 59. 34. 20. 14. 74. 14. 13.  4. 22. 66. 64. 53. 95. 79. 17. 56.
     92. 52. 27. 39. 14. 81. 29.  4. 88.  6. 78. 11. 91. 79. 33. 53. 63. 42.
     61. 82. 32. 20.  2. 58. 92. 78. 92. 67. 99. 45.  6. 16.]




.. GENERATED FROM PYTHON SOURCE LINES 34-53

.. code-block:: Python


    # Now we fit back the PRF parameters
    from braincoder.optimize import ParameterFitter, ResidualFitter
    fitter = ParameterFitter(model, data, paradigm)
    mu_grid = np.arange(0, 100, 5)
    sd_grid = np.arange(5, 50, 5)

    grid_pars = fitter.fit_grid(mu_grid, sd_grid, [1.0], [0.0], use_correlation_cost=True, progressbar=False)
    grid_pars = fitter.refine_baseline_and_amplitude(grid_pars)

    for par in ['mu', 'sd', 'amplitude', 'baseline']:
        print(f'Correlation grid-fitted parameter and ground truth for *{par}*: {ss.pearsonr(grid_pars[par], parameters[par])[0]:0.2f}')

    gd_pars = fitter.fit(init_pars=grid_pars, progressbar=False)

    for par in ['mu', 'sd', 'amplitude', 'baseline']:
        print(f'Correlation gradient descent-fitted parameter and ground truth for *{par}*: {ss.pearsonr(grid_pars[par], parameters[par])[0]:0.2f}')






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Working with chunk size of 666666
    Using correlation cost!
      0%|          | 0/1 [00:00<?, ?it/s]    100%|██████████| 1/1 [00:00<00:00,  5.97it/s]    100%|██████████| 1/1 [00:00<00:00,  5.96it/s]
    Original mean r2: -0.6047407984733582
    100.00% of time lines improved
    New mean r2 after OLS: 0.30944448709487915
    Original mean r2: 0.30944448709487915
    0.00% of time lines improved
    New mean r2 after OLS: 0.30944448709487915
    Correlation grid-fitted parameter and ground truth for *mu*: 0.88
    Correlation grid-fitted parameter and ground truth for *sd*: 0.52
    Correlation grid-fitted parameter and ground truth for *amplitude*: 0.83
    Correlation grid-fitted parameter and ground truth for *baseline*: 0.83
    Number of problematic voxels (mask): 0
    Number of voxels remaining (mask): 20
    Correlation gradient descent-fitted parameter and ground truth for *mu*: 0.88
    Correlation gradient descent-fitted parameter and ground truth for *sd*: 0.52
    Correlation gradient descent-fitted parameter and ground truth for *amplitude*: 0.83
    Correlation gradient descent-fitted parameter and ground truth for *baseline*: 0.83




.. GENERATED FROM PYTHON SOURCE LINES 54-62

.. code-block:: Python


    # Now we fit the covariance matrix
    stimulus_range = np.arange(1, 100).astype(np.float32)

    model.init_pseudoWWT(stimulus_range=stimulus_range, parameters=gd_pars)
    resid_fitter = ResidualFitter(model, data, paradigm, gd_pars)
    omega, dof = resid_fitter.fit(progressbar=False)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    init_tau: 0.7912291884422302, 1.1512037515640259
    USING A PSEUDO-WWT!
    WWT max: 838.3524169921875




.. GENERATED FROM PYTHON SOURCE LINES 63-71

.. code-block:: Python


    # Now we simulate unseen test data:
    test_paradigm = np.ceil(np.random.rand(n_trials) * 100)
    test_data = model.simulate(paradigm=test_paradigm, noise=noise)

    # And decode the test paradigm
    posterior = model.get_stimulus_pdf(test_data, stimulus_range, model.parameters, omega=omega, dof=dof)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [13. 99. 23. 56. 98. 89. 14. 28. 79. 49. 29. 66. 56. 41. 99.  8. 15. 25.
     26. 10. 72. 11. 46.  2. 86. 51. 88. 51. 12.  5.  4. 60. 91. 49. 94. 18.
      8. 95. 69. 67. 49. 26. 33. 83. 25. 12. 23. 42. 96. 52.]
    (99,)




.. GENERATED FROM PYTHON SOURCE LINES 72-88

.. code-block:: Python


    # Finally, we make some plots to see how well the decoder did
    import matplotlib.pyplot as plt
    import seaborn as sns

    tmp = posterior.set_index(pd.Series(test_paradigm, name='ground truth'), append=True).loc[:8].stack().to_frame('p')

    g = sns.FacetGrid(tmp.reset_index(), col='frame', col_wrap=3)

    g.map(plt.plot, 'stimulus', 'p', color='k')

    def test(data, **kwargs):
        plt.axvline(data.mean(), c='k', ls='--', **kwargs)
    g.map(test, 'ground truth')
    g.set(xlabel='Stimulus value', ylabel='Posterior probability density')




.. image-sg:: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_001.png
   :alt: frame = 0, frame = 1, frame = 2, frame = 3, frame = 4, frame = 5, frame = 6, frame = 7, frame = 8
   :srcset: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <seaborn.axisgrid.FacetGrid object at 0x2d0982470>



.. GENERATED FROM PYTHON SOURCE LINES 89-121

.. code-block:: Python


    # Let's look at the summary statistics of the posteriors posteriors
    def get_posterior_stats(posterior):
        # Take integral over the posterior to get to the expectation (mean posterior)
        E = np.trapz(posterior*posterior.columns.values[np.newaxis,:], posterior.columns, axis=1)
    
        # Take the integral over the posterior to get the expectation of the distance to the 
        # mean posterior (i.e., standard deviation)
        sd = np.trapz(np.abs(E[:, np.newaxis] - posterior.columns.astype(float).values[np.newaxis, :]) * posterior, posterior.columns, axis=1)

        stats = pd.DataFrame({'E':E, 'sd':sd}, index=posterior.index)
        return stats

    posterior_stats = get_posterior_stats(posterior)

    # Let's see how far the posterior mean is from the ground truth
    plt.errorbar(test_paradigm, posterior_stats['E'],posterior_stats['sd'], fmt='o',)
    plt.plot([0, 100], [0,100], c='k', ls='--')

    plt.xlabel('Ground truth')
    plt.ylabel('Mean posterior')

    # Let's see how the error depends on the standard deviation of the posterior
    error = test_paradigm - posterior_stats['E']
    error_abs = np.abs(error)
    error_abs.name = 'error'

    sns.lmplot(x='sd', y='error', data=posterior_stats.join(error_abs))

    plt.xlabel('Standard deviation of posterior')
    plt.ylabel('Objective error')




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_002.png
         :alt: decode
         :srcset: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_003.png
         :alt: decode
         :srcset: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_003.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Text(29.0, 0.5, 'Objective error')



.. GENERATED FROM PYTHON SOURCE LINES 122-131

.. code-block:: Python



    # Now, let's try to find the MAP estimate using gradient descent
    from braincoder.optimize import StimulusFitter
    stimulus_fitter = StimulusFitter(model=model, data=test_data, omega=omega)

    # We start with a very coarse grid search, so we are sure we are in the right ballpark
    estimated_stimuli_grid = stimulus_fitter.fit_grid(np.arange(1, 100, 5))








.. GENERATED FROM PYTHON SOURCE LINES 132-143

.. code-block:: Python



    # We can then refine the estimate using gradient descent
    estimated_stimuli_gd = stimulus_fitter.fit(init_pars=estimated_stimuli_grid, progressbar=False)

    # Let's see how well we did
    plt.scatter(test_paradigm, estimated_stimuli_grid, alpha=.5, label='MAP (grid search)')
    plt.scatter(test_paradigm, estimated_stimuli_gd, alpha=.5, label='MAP (gradient descent)')
    plt.scatter(test_paradigm, posterior_stats['E'], alpha=.5, label='Mean posterior')
    plt.plot([0, 100], [0,100], c='k', ls='--', label='Identity line')
    plt.legend()
    # %%


.. image-sg:: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_004.png
   :alt: decode
   :srcset: /auto_examples/00_encodingdecoding/images/sphx_glr_decode_004.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/1000 [00:00<?, ?it/s]      0%|          | 1/1000 [00:00<02:43,  6.11it/s]      5%|▌         | 53/1000 [00:00<00:03, 243.81it/s]     11%|█         | 108/1000 [00:00<00:02, 364.52it/s]     17%|█▋        | 167/1000 [00:00<00:01, 444.28it/s]     22%|██▏       | 223/1000 [00:00<00:01, 481.61it/s]     28%|██▊       | 282/1000 [00:00<00:01, 515.91it/s]     34%|███▍      | 341/1000 [00:00<00:01, 538.57it/s]     40%|████      | 401/1000 [00:00<00:01, 556.04it/s]     46%|████▌     | 459/1000 [00:00<00:00, 563.36it/s]     52%|█████▏    | 519/1000 [00:01<00:00, 572.40it/s]     58%|█████▊    | 578/1000 [00:01<00:00, 575.29it/s]     64%|██████▎   | 636/1000 [00:01<00:00, 574.99it/s]     69%|██████▉   | 694/1000 [00:01<00:00, 569.49it/s]     75%|███████▌  | 754/1000 [00:01<00:00, 576.23it/s]     82%|████████▏ | 815/1000 [00:01<00:00, 583.61it/s]     87%|████████▋ | 874/1000 [00:01<00:00, 579.71it/s]     93%|█████████▎| 933/1000 [00:01<00:00, 581.38it/s]     99%|█████████▉| 992/1000 [00:01<00:00, 579.61it/s]    100%|██████████| 1000/1000 [00:01<00:00, 525.81it/s]

    <matplotlib.legend.Legend object at 0x2ce22c370>




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 14.345 seconds)


.. _sphx_glr_download_auto_examples_00_encodingdecoding_decode.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: decode.ipynb <decode.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: decode.py <decode.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
