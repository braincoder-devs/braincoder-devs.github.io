{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decoding of stimuli from neural data\n\nHere we will simulate neural data given a ground truth encoding model\n and try to decode the stimulus from the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set up a neural model\nfrom braincoder.models import GaussianPRF\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss\n\n# Set up 100 random of PRF parameters\nn = 20\nn_trials = 50\nnoise = 1.\n\nmu = np.random.rand(n) * 100\nsd = np.random.rand(n) * 45 + 5\namplitude = np.random.rand(n) * 5\nbaseline = np.random.rand(n) * 2 - 1\n\nparameters = pd.DataFrame({'mu':mu, 'sd':sd, 'amplitude':amplitude, 'baseline':baseline})\n\n# We have a paradigm of random numbers between 0 and 100\nparadigm = np.ceil(np.random.rand(n_trials) * 100)\n\nmodel = GaussianPRF(parameters=parameters)\ndata = model.simulate(paradigm=paradigm, noise=noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now we fit back the PRF parameters\nfrom braincoder.optimize import ParameterFitter, ResidualFitter\nfitter = ParameterFitter(model, data, paradigm)\nmu_grid = np.arange(0, 100, 5)\nsd_grid = np.arange(5, 50, 5)\n\ngrid_pars = fitter.fit_grid(mu_grid, sd_grid, [1.0], [0.0], use_correlation_cost=True, progressbar=False)\ngrid_pars = fitter.refine_baseline_and_amplitude(grid_pars)\n\nfor par in ['mu', 'sd', 'amplitude', 'baseline']:\n    print(f'Correlation grid-fitted parameter and ground truth for *{par}*: {ss.pearsonr(grid_pars[par], parameters[par])[0]:0.2f}')\n\ngd_pars = fitter.fit(init_pars=grid_pars, progressbar=False)\n\nfor par in ['mu', 'sd', 'amplitude', 'baseline']:\n    print(f'Correlation gradient descent-fitted parameter and ground truth for *{par}*: {ss.pearsonr(grid_pars[par], parameters[par])[0]:0.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now we fit the covariance matrix\nstimulus_range = np.arange(1, 100).astype(np.float32)\n\nmodel.init_pseudoWWT(stimulus_range=stimulus_range, parameters=gd_pars)\nresid_fitter = ResidualFitter(model, data, paradigm, gd_pars)\nomega, dof = resid_fitter.fit(progressbar=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now we simulate unseen test data:\ntest_paradigm = np.ceil(np.random.rand(n_trials) * 100)\ntest_data = model.simulate(paradigm=test_paradigm, noise=noise)\n\n# And decode the test paradigm\nposterior = model.get_stimulus_pdf(test_data, stimulus_range, model.parameters, omega=omega, dof=dof)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Finally, we make some plots to see how well the decoder did\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntmp = posterior.set_index(pd.Series(test_paradigm, name='ground truth'), append=True).loc[:8].stack().to_frame('p')\n\ng = sns.FacetGrid(tmp.reset_index(), col='frame', col_wrap=3)\n\ng.map(plt.plot, 'stimulus', 'p', color='k')\n\ndef test(data, **kwargs):\n    plt.axvline(data.mean(), c='k', ls='--', **kwargs)\ng.map(test, 'ground truth')\ng.set(xlabel='Stimulus value', ylabel='Posterior probability density')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's look at the summary statistics of the posteriors posteriors\ndef get_posterior_stats(posterior, normalize=True):\n    posterior = posterior.copy()\n    posterior = posterior.div(np.trapz(posterior, posterior.columns,axis=1), axis=0)\n\n    # Take integral over the posterior to get to the expectation (mean posterior)\n    E = np.trapz(posterior*posterior.columns.values[np.newaxis,:], posterior.columns, axis=1)\n    \n    # Take the integral over the posterior to get the expectation of the distance to the \n    # mean posterior (i.e., standard deviation)\n    sd = np.trapz(np.abs(E[:, np.newaxis] - posterior.columns.astype(float).values[np.newaxis, :]) * posterior, posterior.columns, axis=1)\n\n    stats = pd.DataFrame({'E':E, 'sd':sd}, index=posterior.index)\n    return stats\n\nposterior_stats = get_posterior_stats(posterior)\n\n# Let's see how far the posterior mean is from the ground truth\nplt.errorbar(test_paradigm, posterior_stats['E'],posterior_stats['sd'], fmt='o',)\nplt.plot([0, 100], [0,100], c='k', ls='--')\n\nplt.xlabel('Ground truth')\nplt.ylabel('Mean posterior')\n\n# Let's see how the error depends on the standard deviation of the posterior\nerror = test_paradigm - posterior_stats['E']\nerror_abs = np.abs(error)\nerror_abs.name = 'error'\n\nsns.lmplot(x='sd', y='error', data=posterior_stats.join(error_abs))\n\nplt.xlabel('Standard deviation of posterior')\nplt.ylabel('Objective error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Now, let's try to find the MAP estimate using gradient descent\nfrom braincoder.optimize import StimulusFitter\nstimulus_fitter = StimulusFitter(model=model, data=test_data, omega=omega)\n\n# We start with a very coarse grid search, so we are sure we are in the right ballpark\nestimated_stimuli_grid = stimulus_fitter.fit_grid(np.arange(1, 100, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can then refine the estimate using gradient descent\nestimated_stimuli_gd = stimulus_fitter.fit(init_pars=estimated_stimuli_grid, progressbar=False)\n\n# Let's see how well we did\nplt.scatter(test_paradigm, estimated_stimuli_grid, alpha=.5, label='MAP (grid search)')\nplt.scatter(test_paradigm, estimated_stimuli_gd, alpha=.5, label='MAP (gradient descent)')\nplt.scatter(test_paradigm, posterior_stats['E'], alpha=.5, label='Mean posterior')\nplt.plot([0, 100], [0,100], c='k', ls='--', label='Identity line')\nplt.legend()\n# %%"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}